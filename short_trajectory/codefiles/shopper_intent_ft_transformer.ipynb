{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tabtransformertf\n!pip install tensorflow-addons","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-02T13:33:01.164811Z","iopub.execute_input":"2023-08-02T13:33:01.165209Z","iopub.status.idle":"2023-08-02T13:33:28.139775Z","shell.execute_reply.started":"2023-08-02T13:33:01.165175Z","shell.execute_reply":"2023-08-02T13:33:28.138390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\nfrom sklearn.neighbors import *\nfrom sklearn.ensemble import *\nfrom sklearn.tree import *\nfrom sklearn.linear_model import *\nfrom sklearn.svm import *\nfrom sklearn.decomposition import *\n\n# import torch\n# import torch.nn as nn\n# from tab_transformer_pytorch import TabTransformer\n\nfrom tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\nfrom tabtransformertf.models.tabtransformer import TabTransformer\nfrom tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport re\nimport ast","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:33:28.143671Z","iopub.execute_input":"2023-08-02T13:33:28.144032Z","iopub.status.idle":"2023-08-02T13:33:45.341715Z","shell.execute_reply.started":"2023-08-02T13:33:28.143998Z","shell.execute_reply":"2023-08-02T13:33:45.340714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length_text = 11\ndirectory_dataframes = '/kaggle/input/short-trajec-subsamples-features/subsamples/subsamples/{}/'.format(length_text)\ndirectory_features = '/kaggle/input/short-trajec-subsamples-features/features/features/{}/'.format(length_text)\n\ndef get_sample_df(directory=directory_dataframes):\n    list_dataframes = []\n    filename_list = []\n    for filename in os.listdir(directory):\n        print(filename)\n        filename_list.append(filename)\n        f = os.path.join(directory, filename)\n        if os.path.isfile(f):\n            list_dataframes.append(pd.read_csv(f))\n            \n    return list_dataframes, filename_list\n\ndef get_features(regex_str, directory=directory_features):\n    regex = re.compile('/kaggle/input/short-trajec-subsamples-features/features/features/{}/{}'.format(length_text, regex_str))\n    \n    for filename in os.listdir(directory):\n        f = os.path.join(directory, filename)\n        if regex.match(f):\n            file1 = open(f,\"r\")\n            feat_list = file1.read().splitlines()\n            \n            #txt file converts everything to string, so we need to convert it back to list\n            for i in range(len(feat_list)):\n                #adding ; to be used a separator for list\n                if i<len(feat_list):\n                    new_val = feat_list[i].replace('y','y;').replace(') ','); ').replace('4 ', '4; ').replace('5 ', '5; ')\n                    feat_list[i] = new_val\n                \n    for val in feat_list:\n        #separating the string into a list of features\n        new_val = val.split('; ')\n        feat_list[feat_list.index(val)] = new_val\n        \n    return feat_list\n\nlist_sample_dataframes, filename_sample_list = get_sample_df(directory_dataframes)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:33:45.342993Z","iopub.execute_input":"2023-08-02T13:33:45.344744Z","iopub.status.idle":"2023-08-02T13:33:49.511390Z","shell.execute_reply.started":"2023-08-02T13:33:45.344703Z","shell.execute_reply":"2023-08-02T13:33:49.510410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_train_predict_ft(regex_str, dataframes=list_sample_dataframes):\n    \n    feat_list = get_features(regex_str)\n    accuracy_list = []\n    f1_score_list = []\n    auc_list = []\n\n    for sample, feat in zip(dataframes, feat_list):\n        feat[len(feat)-1] = feat[len(feat)-1].replace('y;', 'y')\n        \n        CATEGORICAL_FEATURES = [] \n        NUMERIC_FEATURES = feat\n        FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n        LABEL = 'conversion_class'\n        # print(FEATURES)\n        \n        train_data, test_data = train_test_split(sample, test_size=0.2, random_state=42, stratify=sample['conversion_class'])\n        \n        train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n        test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n\n        train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n        test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)\n        \n        # # Transform to TF dataset\n        train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL, batch_size=32)\n        test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], LABEL, shuffle=False, batch_size=32)\n        \n        # print(train_dataset)\n\n        ft_linear_encoder = FTTransformerEncoder(\n            numerical_features = NUMERIC_FEATURES,  # list of numeric features\n            categorical_features = CATEGORICAL_FEATURES,  # list of numeric features\n            numerical_data = train_data[NUMERIC_FEATURES],  # pandas dataframe of numeric features\n            categorical_data = train_data[CATEGORICAL_FEATURES],  # pandas dataframe of categorical features\n            # categorical_lookup=category_prep_layers,  # dictionary of categorical lookup layers\n            # numerical_embeddings=None,  # None for linear embeddings\n            numerical_embedding_type='linear',  # Numerical embedding type\n            embedding_dim=16,  # Embedding dimension (for categorical, numerical, and contextual)\n            depth=3,  # Number of Transformer Blocks (layers)\n            heads=6,  # Number of attention heads in a Transofrmer Block\n            attn_dropout=0.2,  # Dropout for attention layers\n            ff_dropout=0.2,  # Dropout in Dense layers\n            # use_column_embedding=True,  # Fixed column embeddings\n            explainable=True  # Whether we want to output attention importances or not\n        )\n\n        # Pass the encoder to the model\n        ft_linear_transformer = FTTransformer(\n            encoder=ft_linear_encoder,  # Encoder from above\n            out_dim=1,  # Number of outputs in final layer\n            out_activation='sigmoid',  # Activation function for final layer\n            # final_layer_size=32,  # Pre-final layer, takes CLS contextual embeddings as input \n        )\n        \n        LEARNING_RATE = 0.0001\n        WEIGHT_DECAY = 0.0001\n        NUM_EPOCHS = 10\n\n        # Define optimised\n        optimizer = tfa.optimizers.AdamW(\n                learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n            )\n\n        # Two outputs, second output is importances, so we don't calculate loss for it\n        ft_linear_transformer.compile(\n            optimizer = optimizer,\n            loss = {\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n            metrics= {\"output\": [tf.keras.metrics.AUC(name=\"PR AUC\", curve='PR')], \"importances\": None},\n        )\n\n        # Training\n        ft_linear_history = ft_linear_transformer.fit(\n            train_dataset, \n            epochs=NUM_EPOCHS, \n            verbose=0          \n        )\n        \n        \n        linear_test_preds = ft_linear_transformer.predict(test_dataset)\n        auc_list.append(np.round(roc_auc_score(test_data[LABEL], linear_test_preds['output'].ravel()), 4))\n        f1_score_list.append(np.round(f1_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n        accuracy_list.append(np.round(accuracy_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n\n        # print('Average Accuracy', np.mean(accuracy_list))\n        # print('Average F1 Score', np.mean(f1_score_list))\n        # print('Average AUC', np.mean(auc_list)) \n        \n        # print('Max Accuracy', max(accuracy_list))\n        # print('Max F1 Score', max(f1_score_list))\n        # print('Max AUC', max(auc_list))  \n        \n        # best_accuracy_index = accuracy_list.index(max(accuracy_list))\n        # best_f1_score_index = f1_score_list.index(max(f1_score_list))\n        # best_auc_index = auc_list.index(max(auc_list))\n        \n        # print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n        # print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n        # print('Best Sample Index based on Max AUC', best_auc_index)\n        \n        # print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n        # print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n        # print('Best Features based on Max AUC', feat_list[best_auc_index]) \n\n    return accuracy_list, f1_score_list, auc_list,\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:33:49.513502Z","iopub.execute_input":"2023-08-02T13:33:49.513914Z","iopub.status.idle":"2023-08-02T13:33:49.535576Z","shell.execute_reply.started":"2023-08-02T13:33:49.513880Z","shell.execute_reply":"2023-08-02T13:33:49.534671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_train_predict_ft_pca(k, dataframes=list_sample_dataframes):\n    \n    accuracy_list = []\n    f1_score_list = []\n    auc_list = []\n\n    for sample in dataframes:\n        x = sample.drop(['Unnamed: 0', 'conversion_class'], axis=1)\n        y = sample['conversion_class']\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=sample['conversion_class'])\n        \n        pca = PCA(n_components=k)\n        x_train = pca.fit_transform(x_train)\n        x_test = pca.transform(x_test)\n        \n        train_data = pd.DataFrame(x_train, columns=[i for i in range(0,pca.components_.shape[0])])\n        test_data = pd.DataFrame(x_test, columns=[i for i in range(0,pca.components_.shape[0])])\n        \n        CATEGORICAL_FEATURES = [] \n        NUMERIC_FEATURES = list(train_data.columns)\n        FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n        LABEL = 'conversion_class'\n        \n        train_data['conversion_class'] = list(y_train)\n        test_data['conversion_class'] = list(y_test)\n\n        train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n        test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n\n        train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n        test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)\n        \n        # # Transform to TF dataset\n        train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL, batch_size=32)\n        test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], LABEL, shuffle=False, batch_size=32)\n        \n        # print(train_dataset)\n\n        ft_linear_encoder = FTTransformerEncoder(\n            numerical_features = NUMERIC_FEATURES,  # list of numeric features\n            categorical_features = CATEGORICAL_FEATURES,  # list of numeric features\n            numerical_data = train_data[NUMERIC_FEATURES],  # pandas dataframe of numeric features\n            categorical_data = train_data[CATEGORICAL_FEATURES],  # pandas dataframe of categorical features\n            # categorical_lookup=category_prep_layers,  # dictionary of categorical lookup layers\n            # numerical_embeddings=None,  # None for linear embeddings\n            numerical_embedding_type='linear',  # Numerical embedding type\n            embedding_dim=32,  # Embedding dimension (for categorical, numerical, and contextual)\n            depth=4,  # Number of Transformer Blocks (layers)\n            heads=6,  # Number of attention heads in a Transofrmer Block\n            attn_dropout=0.2,  # Dropout for attention layers\n            ff_dropout=0.2,  # Dropout in Dense layers\n            # use_column_embedding=True,  # Fixed column embeddings\n            explainable=True  # Whether we want to output attention importances or not\n        )\n\n        # Pass the encoder to the model\n        ft_linear_transformer = FTTransformer(\n            encoder=ft_linear_encoder,  # Encoder from above\n            out_dim=1,  # Number of outputs in final layer\n            out_activation='sigmoid',  # Activation function for final layer\n            # final_layer_size=32,  # Pre-final layer, takes CLS contextual embeddings as input \n        )\n        \n        LEARNING_RATE = 0.0001\n        WEIGHT_DECAY = 0.0001\n        NUM_EPOCHS = 20\n\n        # Define optimised\n        optimizer = tfa.optimizers.AdamW(\n                learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n            )\n\n        # Two outputs, second output is importances, so we don't calculate loss for it\n        ft_linear_transformer.compile(\n            optimizer = optimizer,\n            loss ={\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n            metrics={\"output\": [tf.keras.metrics.Accuracy()], \"importances\": None},\n        )\n\n        # Training\n        ft_linear_history = ft_linear_transformer.fit(\n            train_dataset, \n            epochs=NUM_EPOCHS, \n            verbose=0          \n        )\n        \n        \n        linear_test_preds = ft_linear_transformer.predict(test_dataset)\n        auc_list.append(np.round(roc_auc_score(test_data[LABEL], linear_test_preds['output'].ravel()), 4))\n        f1_score_list.append(np.round(f1_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n        accuracy_list.append(np.round(accuracy_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n\n#         print('Average Accuracy', np.mean(accuracy_list))\n#         print('Average F1 Score', np.mean(f1_score_list))\n#         print('Average AUC', np.mean(auc_list)) \n        \n#         print('Max Accuracy', max(accuracy_list))\n#         print('Max F1 Score', max(f1_score_list))\n#         print('Max AUC', max(auc_list))  \n        \n#         best_accuracy_index = accuracy_list.index(max(accuracy_list))\n#         best_f1_score_index = f1_score_list.index(max(f1_score_list))\n#         best_auc_index = auc_list.index(max(auc_list))\n        \n#         print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n#         print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n#         print('Best Sample Index based on Max AUC', best_auc_index)\n        \n#         print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n#         print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n#         print('Best Features based on Max AUC', feat_list[best_auc_index]) \n\n    return accuracy_list, f1_score_list, auc_list,","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:39:40.670076Z","iopub.execute_input":"2023-08-02T13:39:40.670467Z","iopub.status.idle":"2023-08-02T13:39:40.694725Z","shell.execute_reply.started":"2023-08-02T13:39:40.670434Z","shell.execute_reply":"2023-08-02T13:39:40.693725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy_list_10_mi, f1_score_list_10_mi, auc_list_10_mi = model_train_predict_ft('mi_feat_list_10*',)\n# accuracy_list_20_mi, f1_score_list_20_mi, auc_list_20_mi = model_train_predict_ft('mi_feat_list_20*',)\n# accuracy_list_30_mi, f1_score_list_30_mi, auc_list_30_mi = model_train_predict_ft('mi_feat_list_30*',)\n# accuracy_list_50_mi, f1_score_list_50_mi, auc_list_50_mi = model_train_predict_ft('mi_feat_list_50*',)\n# accuracy_list_75_mi, f1_score_list_75_mi, auc_list_75_mi = model_train_predict_ft('mi_feat_list_75*',)\n# accuracy_list_90_mi, f1_score_list_90_mi, auc_list_90_mi = model_train_predict_ft('mi_feat_list_90*',)\n\n\n# accuracy_list_10_mrmr, f1_score_list_10_mrmr, auc_list_10_mrmr = model_train_predict_ft('mrmr_feat_list_10*',)\n# accuracy_list_20_mrmr, f1_score_list_20_mrmr, auc_list_20_mrmr = model_train_predict_ft('mrmr_feat_list_20*',)\n# accuracy_list_30_mrmr, f1_score_list_30_mrmr, auc_list_30_mrmr = model_train_predict_ft('mrmr_feat_list_30*',)\n# accuracy_list_50_mrmr, f1_score_list_50_mrmr, auc_list_50_mrmr = model_train_predict_ft('mrmr_feat_list_50*',)\n# accuracy_list_75_mrmr, f1_score_list_75_mrmr, auc_list_75_mrmr = model_train_predict_ft('mrmr_feat_list_75*',)\n# accuracy_list_90_mrmr, f1_score_list_90_mrmr, auc_list_90_mrmr = model_train_predict_ft('mrmr_feat_list_90*',)\n\n\n# accuracy_list_10_mi_mrmr, f1_score_list_10_mi_mrmr, auc_list_10_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_10*',)\n# accuracy_list_20_mi_mrmr, f1_score_list_20_mi_mrmr, auc_list_20_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_20*',)\n# accuracy_list_30_mi_mrmr, f1_score_list_30_mi_mrmr, auc_list_30_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_30*',)\n# accuracy_list_50_mi_mrmr, f1_score_list_50_mi_mrmr, auc_list_50_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_50*',)\n# accuracy_list_75_mi_mrmr, f1_score_list_75_mi_mrmr, auc_list_75_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_75*',)\n# accuracy_list_90_mi_mrmr, f1_score_list_90_mi_mrmr, auc_list_90_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_90*',)\n\n\naccuracy_list_10_pca, f1_score_list_10_pca, auc_list_10_pca = model_train_predict_ft_pca(14,)\naccuracy_list_20_pca, f1_score_list_20_pca, auc_list_20_pca = model_train_predict_ft_pca(28,)\naccuracy_list_30_pca, f1_score_list_30_pca, auc_list_30_pca = model_train_predict_ft_pca(42,)\naccuracy_list_50_pca, f1_score_list_50_pca, auc_list_50_pca = model_train_predict_ft_pca(69,)\naccuracy_list_75_pca, f1_score_list_75_pca, auc_list_75_pca = model_train_predict_ft_pca(104,)\naccuracy_list_90_pca, f1_score_list_90_pca, auc_list_90_pca = model_train_predict_ft_pca(125,)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:39:41.184557Z","iopub.execute_input":"2023-08-02T13:39:41.184914Z","iopub.status.idle":"2023-08-02T13:48:00.608770Z","shell.execute_reply.started":"2023-08-02T13:39:41.184884Z","shell.execute_reply":"2023-08-02T13:48:00.606823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# overall_accuracy_list_mi = (accuracy_list_10_mi + accuracy_list_20_mi + accuracy_list_30_mi + accuracy_list_50_mi + accuracy_list_75_mi + accuracy_list_90_mi)\n# overall_accuracy_list_mrmr = (accuracy_list_10_mrmr + accuracy_list_20_mrmr + accuracy_list_30_mrmr + accuracy_list_50_mrmr + accuracy_list_75_mrmr + accuracy_list_90_mrmr)\n# overall_accuracy_list_mi_mrmr = (accuracy_list_10_mi_mrmr + accuracy_list_20_mi_mrmr + accuracy_list_30_mi_mrmr + accuracy_list_50_mi_mrmr + accuracy_list_75_mi_mrmr + accuracy_list_90_mi_mrmr)\noverall_accuracy_list_pca = (accuracy_list_10_pca + accuracy_list_20_pca + accuracy_list_30_pca + accuracy_list_50_pca + accuracy_list_75_pca + accuracy_list_90_pca)\n\n# overall_f1_score_list_mi = (f1_score_list_10_mi + f1_score_list_20_mi + f1_score_list_30_mi + f1_score_list_50_mi + f1_score_list_75_mi + f1_score_list_90_mi)\n# overall_f1_score_list_mrmr = (f1_score_list_10_mrmr + f1_score_list_20_mrmr + f1_score_list_30_mrmr + f1_score_list_50_mrmr + f1_score_list_75_mrmr + f1_score_list_90_mrmr)\n# overall_f1_score_list_mi_mrmr = (f1_score_list_10_mi_mrmr + f1_score_list_20_mi_mrmr + f1_score_list_30_mi_mrmr + f1_score_list_50_mi_mrmr + f1_score_list_75_mi_mrmr + f1_score_list_90_mi_mrmr)\noverall_f1_score_list_pca = (f1_score_list_10_pca + f1_score_list_20_pca + f1_score_list_30_pca + f1_score_list_50_pca + f1_score_list_75_pca + f1_score_list_90_pca)\n\n# overall_auc_list_mi = (auc_list_10_mi + auc_list_20_mi + auc_list_30_mi + auc_list_50_mi + auc_list_75_mi + auc_list_90_mi)\n# overall_auc_list_mrmr = (auc_list_10_mrmr + auc_list_20_mrmr + auc_list_30_mrmr + auc_list_50_mrmr + auc_list_75_mrmr + auc_list_90_mrmr)\n# overall_auc_list_mi_mrmr = (auc_list_10_mi_mrmr + auc_list_20_mi_mrmr + auc_list_30_mi_mrmr + auc_list_50_mi_mrmr + auc_list_75_mi_mrmr + auc_list_90_mi_mrmr)\noverall_auc_list_pca = (auc_list_10_pca + auc_list_20_pca + auc_list_30_pca + auc_list_50_pca + auc_list_75_pca + auc_list_90_pca)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = ['ft_transformer'] * 60\npercentiles = ['10', '20', '30', '50', '75', '90']\npercentiles = [value for value in percentiles for _ in range(10)]\nfilename_sample_list_final = filename_sample_list * 6\n\nprint(len(models))\nprint(len(percentiles))\nprint(len(filename_sample_list_final))\n# print(len(overall_accuracy_list_mi_mrmr))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T13:31:24.228687Z","iopub.execute_input":"2023-08-02T13:31:24.229340Z","iopub.status.idle":"2023-08-02T13:31:24.235771Z","shell.execute_reply.started":"2023-08-02T13:31:24.229303Z","shell.execute_reply":"2023-08-02T13:31:24.234758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_dictionary = {\n    'samples': filename_sample_list_final,\n    'models': models,\n    'percentiles': percentiles,\n#     'mi_accuracy': overall_accuracy_list_mi,\n#     'mi_f1_score': overall_f1_score_list_mi,\n#     'mi_auc': overall_auc_list_mi,\n#     'mrmr_accuracy': overall_accuracy_list_mrmr,\n#     'mrmr_f1_score': overall_f1_score_list_mrmr,\n#     'mrmr_auc': overall_auc_list_mrmr,\n#     'mi_mrmr_accuracy': overall_accuracy_list_mi_mrmr,\n#     'mi_mrmr_f1_score': overall_f1_score_list_mi_mrmr,\n#     'mi_mrmr_auc': overall_auc_list_mi_mrmr,\n    'pca_accuracy': overall_accuracy_list_pca,\n    'pca_f1_score': overall_f1_score_list_pca,\n    'pca_auc': overall_auc_list_pca,\n}\n\nresults_df = pd.DataFrame(results_dictionary)\nresults_df.to_csv('/kaggle/working/overall_results_20_ft_{}_pca.csv'.format(length_text), index=False)","metadata":{},"execution_count":null,"outputs":[]}]}