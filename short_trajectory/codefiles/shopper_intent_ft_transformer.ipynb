{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:22:54.489544Z","iopub.status.busy":"2023-07-31T06:22:54.489256Z","iopub.status.idle":"2023-07-31T06:23:20.991438Z","shell.execute_reply":"2023-07-31T06:23:20.990173Z","shell.execute_reply.started":"2023-07-31T06:22:54.489518Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tabtransformertf\n","  Downloading tabtransformertf-0.0.8-py3-none-any.whl (16 kB)\n","Requirement already satisfied: tensorflow>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from tabtransformertf) (2.12.0)\n","Requirement already satisfied: pandas>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from tabtransformertf) (1.5.3)\n","Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from tabtransformertf) (1.23.5)\n","Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from tabtransformertf) (4.65.0)\n","Requirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from tabtransformertf) (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.1->tabtransformertf) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.1->tabtransformertf) (2023.3)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.0->tabtransformertf) (1.11.1)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.0->tabtransformertf) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.0->tabtransformertf) (3.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (1.51.1)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (21.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (3.20.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (59.8.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (4.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.6.2->tabtransformertf) (0.31.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.2->tabtransformertf) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.6.2->tabtransformertf) (0.2.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (2.20.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (2.3.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.6.2->tabtransformertf) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (2023.5.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.6.2->tabtransformertf) (3.2.2)\n","Installing collected packages: tabtransformertf\n","Successfully installed tabtransformertf-0.0.8\n","Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}],"source":["!pip install tabtransformertf\n","!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:23:20.995235Z","iopub.status.busy":"2023-07-31T06:23:20.993968Z","iopub.status.idle":"2023-07-31T06:23:31.352915Z","shell.execute_reply":"2023-07-31T06:23:31.351899Z","shell.execute_reply.started":"2023-07-31T06:23:20.995193Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from sklearn.model_selection import *\n","from sklearn.metrics import *\n","\n","from sklearn.neighbors import *\n","from sklearn.ensemble import *\n","from sklearn.tree import *\n","from sklearn.linear_model import *\n","from sklearn.svm import *\n","from sklearn.decomposition import *\n","\n","# import torch\n","# import torch.nn as nn\n","# from tab_transformer_pytorch import TabTransformer\n","\n","from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n","from tabtransformertf.models.tabtransformer import TabTransformer\n","from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","import re\n","import ast"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:34:32.931313Z","iopub.status.busy":"2023-07-31T06:34:32.930877Z","iopub.status.idle":"2023-07-31T06:34:35.212754Z","shell.execute_reply":"2023-07-31T06:34:35.211769Z","shell.execute_reply.started":"2023-07-31T06:34:32.931280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["subsample_7.csv\n","subsample_10.csv\n","subsample_8.csv\n","subsample_3.csv\n","subsample_1.csv\n","subsample_2.csv\n","subsample_6.csv\n","subsample_5.csv\n","subsample_4.csv\n","subsample_9.csv\n"]}],"source":["length_text = 14\n","directory_dataframes = '/kaggle/input/short-trajec-subsamples-features/subsamples/subsamples/{}/'.format(length_text)\n","directory_features = '/kaggle/input/short-trajec-subsamples-features/features/features/{}/'.format(length_text)\n","\n","def get_sample_df(directory=directory_dataframes):\n","    list_dataframes = []\n","    filename_list = []\n","    for filename in os.listdir(directory):\n","        print(filename)\n","        filename_list.append(filename)\n","        f = os.path.join(directory, filename)\n","        if os.path.isfile(f):\n","            list_dataframes.append(pd.read_csv(f))\n","            \n","    return list_dataframes, filename_list\n","\n","def get_features(regex_str, directory=directory_features):\n","    regex = re.compile('/kaggle/input/short-trajec-subsamples-features/features/features/{}/{}'.format(length_text, regex_str))\n","    \n","    for filename in os.listdir(directory):\n","        f = os.path.join(directory, filename)\n","        if regex.match(f):\n","            file1 = open(f,\"r\")\n","            feat_list = file1.read().splitlines()\n","            \n","            #txt file converts everything to string, so we need to convert it back to list\n","            for i in range(len(feat_list)):\n","                #adding ; to be used a separator for list\n","                if i<len(feat_list):\n","                    new_val = feat_list[i].replace('y','y;').replace(') ','); ').replace('4 ', '4; ').replace('5 ', '5; ')\n","                    feat_list[i] = new_val\n","                \n","    for val in feat_list:\n","        #separating the string into a list of features\n","        new_val = val.split('; ')\n","        feat_list[feat_list.index(val)] = new_val\n","        \n","    return feat_list\n","\n","list_sample_dataframes, filename_sample_list = get_sample_df(directory_dataframes)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:34:35.215288Z","iopub.status.busy":"2023-07-31T06:34:35.214919Z","iopub.status.idle":"2023-07-31T06:34:35.234216Z","shell.execute_reply":"2023-07-31T06:34:35.233139Z","shell.execute_reply.started":"2023-07-31T06:34:35.215256Z"},"trusted":true},"outputs":[],"source":["def model_train_predict_ft(regex_str, dataframes=list_sample_dataframes):\n","    \n","    feat_list = get_features(regex_str)\n","    accuracy_list = []\n","    f1_score_list = []\n","    auc_list = []\n","\n","    for sample, feat in zip(dataframes, feat_list):\n","        feat[len(feat)-1] = feat[len(feat)-1].replace('y;', 'y')\n","        \n","        CATEGORICAL_FEATURES = [] \n","        NUMERIC_FEATURES = feat\n","        FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n","        LABEL = 'conversion_class'\n","        # print(FEATURES)\n","        \n","        train_data, test_data = train_test_split(sample, test_size=0.2, random_state=42, stratify=sample['conversion_class'])\n","        \n","        train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n","        test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n","\n","        train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n","        test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)\n","        \n","        # # Transform to TF dataset\n","        train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL, batch_size=32)\n","        test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], LABEL, shuffle=False, batch_size=32)\n","        \n","        # print(train_dataset)\n","\n","        ft_linear_encoder = FTTransformerEncoder(\n","            numerical_features = NUMERIC_FEATURES,  # list of numeric features\n","            categorical_features = CATEGORICAL_FEATURES,  # list of numeric features\n","            numerical_data = train_data[NUMERIC_FEATURES],  # pandas dataframe of numeric features\n","            categorical_data = train_data[CATEGORICAL_FEATURES],  # pandas dataframe of categorical features\n","            # categorical_lookup=category_prep_layers,  # dictionary of categorical lookup layers\n","            # numerical_embeddings=None,  # None for linear embeddings\n","            numerical_embedding_type='linear',  # Numerical embedding type\n","            embedding_dim=16,  # Embedding dimension (for categorical, numerical, and contextual)\n","            depth=3,  # Number of Transformer Blocks (layers)\n","            heads=6,  # Number of attention heads in a Transofrmer Block\n","            attn_dropout=0.2,  # Dropout for attention layers\n","            ff_dropout=0.2,  # Dropout in Dense layers\n","            # use_column_embedding=True,  # Fixed column embeddings\n","            explainable=True  # Whether we want to output attention importances or not\n","        )\n","\n","        # Pass the encoder to the model\n","        ft_linear_transformer = FTTransformer(\n","            encoder=ft_linear_encoder,  # Encoder from above\n","            out_dim=1,  # Number of outputs in final layer\n","            out_activation='sigmoid',  # Activation function for final layer\n","            # final_layer_size=32,  # Pre-final layer, takes CLS contextual embeddings as input \n","        )\n","        \n","        LEARNING_RATE = 0.0001\n","        WEIGHT_DECAY = 0.0001\n","        NUM_EPOCHS = 10\n","\n","        # Define optimised\n","        optimizer = tfa.optimizers.AdamW(\n","                learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","            )\n","\n","        # Two outputs, second output is importances, so we don't calculate loss for it\n","        ft_linear_transformer.compile(\n","            optimizer = optimizer,\n","            loss = {\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n","            metrics= {\"output\": [tf.keras.metrics.AUC(name=\"PR AUC\", curve='PR')], \"importances\": None},\n","        )\n","\n","        # Training\n","        ft_linear_history = ft_linear_transformer.fit(\n","            train_dataset, \n","            epochs=NUM_EPOCHS, \n","            verbose=0          \n","        )\n","        \n","        \n","        linear_test_preds = ft_linear_transformer.predict(test_dataset)\n","        auc_list.append(np.round(roc_auc_score(test_data[LABEL], linear_test_preds['output'].ravel()), 4))\n","        f1_score_list.append(np.round(f1_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n","        accuracy_list.append(np.round(accuracy_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n","\n","        # print('Average Accuracy', np.mean(accuracy_list))\n","        # print('Average F1 Score', np.mean(f1_score_list))\n","        # print('Average AUC', np.mean(auc_list)) \n","        \n","        # print('Max Accuracy', max(accuracy_list))\n","        # print('Max F1 Score', max(f1_score_list))\n","        # print('Max AUC', max(auc_list))  \n","        \n","        # best_accuracy_index = accuracy_list.index(max(accuracy_list))\n","        # best_f1_score_index = f1_score_list.index(max(f1_score_list))\n","        # best_auc_index = auc_list.index(max(auc_list))\n","        \n","        # print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n","        # print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n","        # print('Best Sample Index based on Max AUC', best_auc_index)\n","        \n","        # print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n","        # print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n","        # print('Best Features based on Max AUC', feat_list[best_auc_index]) \n","\n","    return accuracy_list, f1_score_list, auc_list,\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:34:35.237820Z","iopub.status.busy":"2023-07-31T06:34:35.237532Z","iopub.status.idle":"2023-07-31T06:34:35.258015Z","shell.execute_reply":"2023-07-31T06:34:35.257003Z","shell.execute_reply.started":"2023-07-31T06:34:35.237796Z"},"trusted":true},"outputs":[],"source":["def model_train_predict_ft_pca(k, dataframes=list_sample_dataframes):\n","    \n","    accuracy_list = []\n","    f1_score_list = []\n","    auc_list = []\n","\n","    for sample in dataframes:\n","        x = sample.drop(['Unnamed: 0', 'conversion_class'], axis=1)\n","        y = sample['conversion_class']\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=sample['conversion_class'])\n","        \n","        pca = PCA(n_components=k)\n","        x_train = pca.fit_transform(x_train)\n","        x_test = pca.transform(x_test)\n","        \n","        train_data = pd.DataFrame(x_train, columns=[i for i in range(0,pca.components_.shape[0])])\n","        test_data = pd.DataFrame(x_test, columns=[i for i in range(0,pca.components_.shape[0])])\n","        \n","        CATEGORICAL_FEATURES = [] \n","        NUMERIC_FEATURES = list(train_data.columns)\n","        FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n","        LABEL = 'conversion_class'\n","        \n","        train_data['conversion_class'] = list(y_train)\n","        test_data['conversion_class'] = list(y_test)\n","\n","        train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n","        test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n","\n","        train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n","        test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)\n","        \n","        # # Transform to TF dataset\n","        train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL, batch_size=32)\n","        test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], LABEL, shuffle=False, batch_size=32)\n","        \n","        # print(train_dataset)\n","\n","        ft_linear_encoder = FTTransformerEncoder(\n","            numerical_features = NUMERIC_FEATURES,  # list of numeric features\n","            categorical_features = CATEGORICAL_FEATURES,  # list of numeric features\n","            numerical_data = train_data[NUMERIC_FEATURES],  # pandas dataframe of numeric features\n","            categorical_data = train_data[CATEGORICAL_FEATURES],  # pandas dataframe of categorical features\n","            # categorical_lookup=category_prep_layers,  # dictionary of categorical lookup layers\n","            # numerical_embeddings=None,  # None for linear embeddings\n","            numerical_embedding_type='linear',  # Numerical embedding type\n","            embedding_dim=32,  # Embedding dimension (for categorical, numerical, and contextual)\n","            depth=4,  # Number of Transformer Blocks (layers)\n","            heads=6,  # Number of attention heads in a Transofrmer Block\n","            attn_dropout=0.2,  # Dropout for attention layers\n","            ff_dropout=0.2,  # Dropout in Dense layers\n","            # use_column_embedding=True,  # Fixed column embeddings\n","            explainable=True  # Whether we want to output attention importances or not\n","        )\n","\n","        # Pass the encoder to the model\n","        ft_linear_transformer = FTTransformer(\n","            encoder=ft_linear_encoder,  # Encoder from above\n","            out_dim=1,  # Number of outputs in final layer\n","            out_activation='sigmoid',  # Activation function for final layer\n","            # final_layer_size=32,  # Pre-final layer, takes CLS contextual embeddings as input \n","        )\n","        \n","        LEARNING_RATE = 0.0001\n","        WEIGHT_DECAY = 0.0001\n","        NUM_EPOCHS = 20\n","\n","        # Define optimised\n","        optimizer = tfa.optimizers.AdamW(\n","                learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","            )\n","\n","        # Two outputs, second output is importances, so we don't calculate loss for it\n","        ft_linear_transformer.compile(\n","            optimizer = optimizer,\n","            loss ={\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n","            metrics={\"output\": [tf.keras.metrics.Accuracy()], \"importances\": None},\n","        )\n","\n","        # Training\n","        ft_linear_history = ft_linear_transformer.fit(\n","            train_dataset, \n","            epochs=NUM_EPOCHS, \n","            verbose=0          \n","        )\n","        \n","        \n","        linear_test_preds = ft_linear_transformer.predict(test_dataset)\n","        auc_list.append(np.round(roc_auc_score(test_data[LABEL], linear_test_preds['output'].ravel()), 4))\n","        f1_score_list.append(np.round(f1_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n","        accuracy_list.append(np.round(accuracy_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n","\n","#         print('Average Accuracy', np.mean(accuracy_list))\n","#         print('Average F1 Score', np.mean(f1_score_list))\n","#         print('Average AUC', np.mean(auc_list)) \n","        \n","#         print('Max Accuracy', max(accuracy_list))\n","#         print('Max F1 Score', max(f1_score_list))\n","#         print('Max AUC', max(auc_list))  \n","        \n","#         best_accuracy_index = accuracy_list.index(max(accuracy_list))\n","#         best_f1_score_index = f1_score_list.index(max(f1_score_list))\n","#         best_auc_index = auc_list.index(max(auc_list))\n","        \n","#         print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n","#         print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n","#         print('Best Sample Index based on Max AUC', best_auc_index)\n","        \n","#         print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n","#         print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n","#         print('Best Features based on Max AUC', feat_list[best_auc_index]) \n","\n","    return accuracy_list, f1_score_list, auc_list,"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T06:34:35.260711Z","iopub.status.busy":"2023-07-31T06:34:35.260320Z","iopub.status.idle":"2023-07-31T06:38:22.926347Z","shell.execute_reply":"2023-07-31T06:38:22.924637Z","shell.execute_reply.started":"2023-07-31T06:34:35.260672Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["76/76 [==============================] - 1s 8ms/step\n","76/76 [==============================] - 1s 7ms/step\n","76/76 [==============================] - 1s 7ms/step\n","76/76 [==============================] - 1s 8ms/step\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy_list_10_mi, f1_score_list_10_mi, auc_list_10_mi \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train_predict_ft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmi_feat_list_10*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m accuracy_list_20_mi, f1_score_list_20_mi, auc_list_20_mi \u001b[38;5;241m=\u001b[39m model_train_predict_ft(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_feat_list_20*\u001b[39m\u001b[38;5;124m'\u001b[39m,)\n\u001b[1;32m      3\u001b[0m accuracy_list_30_mi, f1_score_list_30_mi, auc_list_30_mi \u001b[38;5;241m=\u001b[39m model_train_predict_ft(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_feat_list_30*\u001b[39m\u001b[38;5;124m'\u001b[39m,)\n","Cell \u001b[0;32mIn[8], line 73\u001b[0m, in \u001b[0;36mmodel_train_predict_ft\u001b[0;34m(regex_str, dataframes)\u001b[0m\n\u001b[1;32m     66\u001b[0m ft_linear_transformer\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     67\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportances\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[1;32m     69\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPR AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, curve\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPR\u001b[39m\u001b[38;5;124m'\u001b[39m)], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportances\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m ft_linear_history \u001b[38;5;241m=\u001b[39m \u001b[43mft_linear_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m          \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m linear_test_preds \u001b[38;5;241m=\u001b[39m ft_linear_transformer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m     81\u001b[0m auc_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mround(roc_auc_score(test_data[LABEL], linear_test_preds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mravel()), \u001b[38;5;241m4\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1674\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1676\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m         ):\n\u001b[1;32m   1684\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1376\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1377\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1380\u001b[0m )\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    646\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    649\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["accuracy_list_10_mi, f1_score_list_10_mi, auc_list_10_mi = model_train_predict_ft('mi_feat_list_10*',)\n","accuracy_list_20_mi, f1_score_list_20_mi, auc_list_20_mi = model_train_predict_ft('mi_feat_list_20*',)\n","accuracy_list_30_mi, f1_score_list_30_mi, auc_list_30_mi = model_train_predict_ft('mi_feat_list_30*',)\n","accuracy_list_50_mi, f1_score_list_50_mi, auc_list_50_mi = model_train_predict_ft('mi_feat_list_50*',)\n","accuracy_list_75_mi, f1_score_list_75_mi, auc_list_75_mi = model_train_predict_ft('mi_feat_list_75*',)\n","accuracy_list_90_mi, f1_score_list_90_mi, auc_list_90_mi = model_train_predict_ft('mi_feat_list_90*',)\n","\n","\n","accuracy_list_10_mrmr, f1_score_list_10_mrmr, auc_list_10_mrmr = model_train_predict_ft('mrmr_feat_list_10*',)\n","accuracy_list_20_mrmr, f1_score_list_20_mrmr, auc_list_20_mrmr = model_train_predict_ft('mrmr_feat_list_20*',)\n","accuracy_list_30_mrmr, f1_score_list_30_mrmr, auc_list_30_mrmr = model_train_predict_ft('mrmr_feat_list_30*',)\n","accuracy_list_50_mrmr, f1_score_list_50_mrmr, auc_list_50_mrmr = model_train_predict_ft('mrmr_feat_list_50*',)\n","accuracy_list_75_mrmr, f1_score_list_75_mrmr, auc_list_75_mrmr = model_train_predict_ft('mrmr_feat_list_75*',)\n","accuracy_list_90_mrmr, f1_score_list_90_mrmr, auc_list_90_mrmr = model_train_predict_ft('mrmr_feat_list_90*',)\n","\n","\n","accuracy_list_10_mi_mrmr, f1_score_list_10_mi_mrmr, auc_list_10_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_10*',)\n","accuracy_list_20_mi_mrmr, f1_score_list_20_mi_mrmr, auc_list_20_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_20*',)\n","accuracy_list_30_mi_mrmr, f1_score_list_30_mi_mrmr, auc_list_30_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_30*',)\n","accuracy_list_50_mi_mrmr, f1_score_list_50_mi_mrmr, auc_list_50_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_50*',)\n","accuracy_list_75_mi_mrmr, f1_score_list_75_mi_mrmr, auc_list_75_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_75*',)\n","accuracy_list_90_mi_mrmr, f1_score_list_90_mi_mrmr, auc_list_90_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_90*',)\n","\n","\n","# accuracy_list_10_pca, f1_score_list_10_pca, auc_list_10_pca = model_train_predict_ft_pca(14,)\n","# accuracy_list_20_pca, f1_score_list_20_pca, auc_list_20_pca = model_train_predict_ft_pca(28,)\n","# accuracy_list_30_pca, f1_score_list_30_pca, auc_list_30_pca = model_train_predict_ft_pca(42,)\n","# accuracy_list_50_pca, f1_score_list_50_pca, auc_list_50_pca = model_train_predict_ft_pca(69,)\n","# accuracy_list_75_pca, f1_score_list_75_pca, auc_list_75_pca = model_train_predict_ft_pca(104,)\n","# accuracy_list_90_pca, f1_score_list_90_pca, auc_list_90_pca = model_train_predict_ft_pca(125,)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-31T06:38:22.950525Z","iopub.status.idle":"2023-07-31T06:38:22.950944Z","shell.execute_reply":"2023-07-31T06:38:22.950774Z","shell.execute_reply.started":"2023-07-31T06:38:22.950755Z"},"trusted":true},"outputs":[],"source":["overall_accuracy_list_mi = (accuracy_list_10_mi + accuracy_list_20_mi + accuracy_list_30_mi + accuracy_list_50_mi + accuracy_list_75_mi + accuracy_list_90_mi)\n","overall_accuracy_list_mrmr = (accuracy_list_10_mrmr + accuracy_list_20_mrmr + accuracy_list_30_mrmr + accuracy_list_50_mrmr + accuracy_list_75_mrmr + accuracy_list_90_mrmr)\n","overall_accuracy_list_mi_mrmr = (accuracy_list_10_mi_mrmr + accuracy_list_20_mi_mrmr + accuracy_list_30_mi_mrmr + accuracy_list_50_mi_mrmr + accuracy_list_75_mi_mrmr + accuracy_list_90_mi_mrmr)\n","# overall_accuracy_list_pca = (accuracy_list_10_pca + accuracy_list_20_pca + accuracy_list_30_pca + accuracy_list_50_pca + accuracy_list_75_pca + accuracy_list_90_pca)\n","\n","overall_f1_score_list_mi = (f1_score_list_10_mi + f1_score_list_20_mi + f1_score_list_30_mi + f1_score_list_50_mi + f1_score_list_75_mi + f1_score_list_90_mi)\n","overall_f1_score_list_mrmr = (f1_score_list_10_mrmr + f1_score_list_20_mrmr + f1_score_list_30_mrmr + f1_score_list_50_mrmr + f1_score_list_75_mrmr + f1_score_list_90_mrmr)\n","overall_f1_score_list_mi_mrmr = (f1_score_list_10_mi_mrmr + f1_score_list_20_mi_mrmr + f1_score_list_30_mi_mrmr + f1_score_list_50_mi_mrmr + f1_score_list_75_mi_mrmr + f1_score_list_90_mi_mrmr)\n","# overall_f1_score_list_pca = (f1_score_list_10_pca + f1_score_list_20_pca + f1_score_list_30_pca + f1_score_list_50_pca + f1_score_list_75_pca + f1_score_list_90_pca)\n","\n","overall_auc_list_mi = (auc_list_10_mi + auc_list_20_mi + auc_list_30_mi + auc_list_50_mi + auc_list_75_mi + auc_list_90_mi)\n","overall_auc_list_mrmr = (auc_list_10_mrmr + auc_list_20_mrmr + auc_list_30_mrmr + auc_list_50_mrmr + auc_list_75_mrmr + auc_list_90_mrmr)\n","overall_auc_list_mi_mrmr = (auc_list_10_mi_mrmr + auc_list_20_mi_mrmr + auc_list_30_mi_mrmr + auc_list_50_mi_mrmr + auc_list_75_mi_mrmr + auc_list_90_mi_mrmr)\n","# overall_auc_list_pca = (auc_list_10_pca + auc_list_20_pca + auc_list_30_pca + auc_list_50_pca + auc_list_75_pca + auc_list_90_pca)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models = ['ft_transformer'] * 60\n","percentiles = ['10', '20', '30', '50', '75', '90'] * 10\n","filename_sample_list_final = filename_sample_list * 6\n","\n","print(len(models))\n","print(len(percentiles))\n","print(len(filename_sample_list_final))\n","print(len(overall_accuracy_list_mi_mrmr))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_dictionary = {\n","    'samples': filename_sample_list_final,\n","    'models': models,\n","    'percentiles': percentiles,\n","    'mi_accuracy': overall_accuracy_list_mi,\n","    'mi_f1_score': overall_f1_score_list_mi,\n","    'mi_auc': overall_auc_list_mi,\n","    'mrmr_accuracy': overall_accuracy_list_mrmr,\n","    'mrmr_f1_score': overall_f1_score_list_mrmr,\n","    'mrmr_auc': overall_auc_list_mrmr,\n","    'mi_mrmr_accuracy': overall_accuracy_list_mi_mrmr,\n","    'mi_mrmr_f1_score': overall_f1_score_list_mi_mrmr,\n","    'mi_mrmr_auc': overall_auc_list_mi_mrmr,\n","}\n","\n","results_df = pd.DataFrame(results_dictionary)\n","results_df.to_csv('/content/overall_results_20_ft_{}.csv'.format(length_text), index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
