{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_selection import *\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80442, 153)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversion_class</th>\n",
       "      <th>unigram_entropy</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>pattern_hvg_4_nodes_entropy</th>\n",
       "      <th>pattern_hvg_5_node_entropy</th>\n",
       "      <th>(1,)</th>\n",
       "      <th>(2,)</th>\n",
       "      <th>(3,)</th>\n",
       "      <th>(6,)</th>\n",
       "      <th>...</th>\n",
       "      <th>R5</th>\n",
       "      <th>T5</th>\n",
       "      <th>J5</th>\n",
       "      <th>C5</th>\n",
       "      <th>D5</th>\n",
       "      <th>H5</th>\n",
       "      <th>K5</th>\n",
       "      <th>F5</th>\n",
       "      <th>M5</th>\n",
       "      <th>I5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.691416</td>\n",
       "      <td>1.255482</td>\n",
       "      <td>1.783854</td>\n",
       "      <td>1.265857</td>\n",
       "      <td>1.671595</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.677494</td>\n",
       "      <td>0.983274</td>\n",
       "      <td>1.171060</td>\n",
       "      <td>1.197340</td>\n",
       "      <td>1.711845</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.971849</td>\n",
       "      <td>1.245017</td>\n",
       "      <td>1.450252</td>\n",
       "      <td>1.479133</td>\n",
       "      <td>2.138333</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.691416</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.690923</td>\n",
       "      <td>1.479133</td>\n",
       "      <td>1.979205</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.971849</td>\n",
       "      <td>1.401393</td>\n",
       "      <td>1.691434</td>\n",
       "      <td>1.377820</td>\n",
       "      <td>2.094729</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversion_class  unigram_entropy  bigram_entropy  trigram_entropy  \\\n",
       "0                 0         0.691416        1.255482         1.783854   \n",
       "1                 0         0.677494        0.983274         1.171060   \n",
       "2                 0         0.971849        1.245017         1.450252   \n",
       "3                 0         0.691416        0.693147         0.690923   \n",
       "4                 0         0.971849        1.401393         1.691434   \n",
       "\n",
       "   pattern_hvg_4_nodes_entropy  pattern_hvg_5_node_entropy      (1,)  \\\n",
       "0                     1.265857                    1.671595  0.529412   \n",
       "1                     1.197340                    1.711845  0.588235   \n",
       "2                     1.479133                    2.138333  0.470588   \n",
       "3                     1.479133                    1.979205  0.470588   \n",
       "4                     1.377820                    2.094729  0.470588   \n",
       "\n",
       "       (2,)      (3,)  (6,)  ...        R5   T5   J5   C5   D5   H5   K5   F5  \\\n",
       "0  0.470588  0.000000   0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.411765  0.000000   0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.411765  0.117647   0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.529412  0.000000   0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.411765  0.117647   0.0  ...  0.083333  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    M5   I5  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_index = 17\n",
    "shop_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/data/short_trajectory/shopper_data_slm_feat_length_{}_v2.csv'.format(length_index))\n",
    "shop_df.drop(['session_id_hash', 'product_action', 'reduced_time', 'HVGms_edges', 'pattern_hvg_4_nodes', 'pattern_hvg_5_nodes', 'unigram', 'bigram', 'trigram', 'unigram_prob_freq', 'bigram_prob_freq', 'trigram_prob_freq', 'pattern_hvg_4_nodes_prob_freq', 'pattern_hvg_5_nodes_prob_freq'], axis=1, inplace=True)\n",
    "shop_df['conversion_class'] = np.where(shop_df['conversion_class'] == 'NC', 0, 1)\n",
    "print(shop_df.shape)\n",
    "shop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n",
      "(11032, 152) (11032,)\n"
     ]
    }
   ],
   "source": [
    "x, y = shop_df.drop('conversion_class', axis=1), shop_df['conversion_class']\n",
    "subsamples = []\n",
    "for i in range(10):\n",
    "    rus = RandomUnderSampler(random_state=i)\n",
    "    x_resampled, y_resampled = rus.fit_resample(x, y)\n",
    "    print(x_resampled.shape, y_resampled.shape)\n",
    "    subsamples.append((x_resampled, y_resampled))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64353, 152) (16089, 152) (64353,) (16089,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI (Mutual Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m y_sample \u001b[39m=\u001b[39m samples[\u001b[39m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m sel_10_percentile_cols \u001b[39m=\u001b[39m SelectPercentile(mutual_info_classif, percentile\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m sel_10_percentile_cols\u001b[39m.\u001b[39;49mfit(x_sample, y_sample)\n\u001b[1;32m     15\u001b[0m mi_feat_list_10_percentile\u001b[39m.\u001b[39mappend(x_sample\u001b[39m.\u001b[39mcolumns[sel_10_percentile_cols\u001b[39m.\u001b[39mget_support()])\n\u001b[1;32m     17\u001b[0m sel_20_percentile_cols \u001b[39m=\u001b[39m SelectPercentile(mutual_info_classif, percentile\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:503\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    498\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    499\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    500\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X, y)\n\u001b[0;32m--> 503\u001b[0m score_func_ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_func(X, y)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(score_func_ret, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscores_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpvalues_ \u001b[39m=\u001b[39m score_func_ret\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:493\u001b[0m, in \u001b[0;36mmutual_info_classif\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Estimate mutual information for a discrete target variable.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[39mMutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39m       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m check_classification_targets(y)\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[39mTrue\u001b[39;49;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:307\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    300\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    302\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[1;32m    304\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[1;32m    308\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[1;32m    309\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    312\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:308\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    300\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    302\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[1;32m    304\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    307\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 308\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[1;32m    309\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    312\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:166\u001b[0m, in \u001b[0;36m_compute_mi\u001b[0;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(y, x, n_neighbors)\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m x_discrete \u001b[39mand\u001b[39;00m y_discrete:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(x, y, n_neighbors)\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cc(x, y, n_neighbors)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shopper-intent-prediction-L5e-nQUd/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:142\u001b[0m, in \u001b[0;36m_compute_mi_cd\u001b[0;34m(c, d, n_neighbors)\u001b[0m\n\u001b[1;32m    139\u001b[0m radius \u001b[39m=\u001b[39m radius[mask]\n\u001b[1;32m    141\u001b[0m kd \u001b[39m=\u001b[39m KDTree(c)\n\u001b[0;32m--> 142\u001b[0m m_all \u001b[39m=\u001b[39m kd\u001b[39m.\u001b[39;49mquery_radius(c, radius, count_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    143\u001b[0m m_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(m_all)\n\u001b[1;32m    145\u001b[0m mi \u001b[39m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m     digamma(n_samples)\n\u001b[1;32m    147\u001b[0m     \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mmean(digamma(k_all))\n\u001b[1;32m    148\u001b[0m     \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmean(digamma(label_counts))\n\u001b[1;32m    149\u001b[0m     \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmean(digamma(m_all))\n\u001b[1;32m    150\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mi_feat_list_10_percentile = []\n",
    "mi_feat_list_20_percentile = []\n",
    "mi_feat_list_30_percentile = []\n",
    "mi_feat_list_50_percentile = []\n",
    "mi_feat_list_75_percentile = []\n",
    "mi_feat_list_90_percentile = []\n",
    "\n",
    "\n",
    "for samples in subsamples:\n",
    "    x_sample = samples[0]\n",
    "    y_sample = samples[1]\n",
    "    \n",
    "    sel_10_percentile_cols = SelectPercentile(mutual_info_classif, percentile=10)\n",
    "    sel_10_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_10_percentile.append(x_sample.columns[sel_10_percentile_cols.get_support()])\n",
    "    \n",
    "    sel_20_percentile_cols = SelectPercentile(mutual_info_classif, percentile=20)\n",
    "    sel_20_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_20_percentile.append(x_sample.columns[sel_20_percentile_cols.get_support()])\n",
    "    \n",
    "    sel_30_percentile_cols = SelectPercentile(mutual_info_classif, percentile=30)\n",
    "    sel_30_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_30_percentile.append(x_sample.columns[sel_30_percentile_cols.get_support()])\n",
    "    \n",
    "    sel_50_percentile_cols = SelectPercentile(mutual_info_classif, percentile=50)\n",
    "    sel_50_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_50_percentile.append(x_sample.columns[sel_50_percentile_cols.get_support()])\n",
    "    \n",
    "    sel_75_percentile_cols = SelectPercentile(mutual_info_classif, percentile=75)\n",
    "    sel_75_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_75_percentile.append(x_sample.columns[sel_75_percentile_cols.get_support()])\n",
    "    \n",
    "    sel_90_percentile_cols = SelectPercentile(mutual_info_classif, percentile=90)\n",
    "    sel_90_percentile_cols.fit(x_sample, y_sample)\n",
    "    mi_feat_list_90_percentile.append(x_sample.columns[sel_90_percentile_cols.get_support()])\n",
    "\n",
    "print(len(mi_feat_list_10_percentile[0]))\n",
    "print(len(mi_feat_list_20_percentile[0]))\n",
    "print(len(mi_feat_list_30_percentile[0]))\n",
    "print(len(mi_feat_list_50_percentile[0]))\n",
    "print(len(mi_feat_list_75_percentile[0]))\n",
    "print(len(mi_feat_list_90_percentile[0]))\n",
    "\n",
    "print(len(mi_feat_list_10_percentile))\n",
    "print(len(mi_feat_list_20_percentile))\n",
    "print(len(mi_feat_list_30_percentile))\n",
    "print(len(mi_feat_list_50_percentile))\n",
    "print(len(mi_feat_list_75_percentile))\n",
    "print(len(mi_feat_list_90_percentile))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr_feat_selected(x, y, f, correlation, k):\n",
    "    feat_list = []\n",
    "    selected = []\n",
    "    not_selected = x.columns.to_list()\n",
    "    # repeat k times\n",
    "    for i in range(k):\n",
    "    \n",
    "        if i > 0:\n",
    "            last_selected = selected[-1]\n",
    "            correlation.loc[not_selected, last_selected] = x[not_selected].corrwith(x[last_selected]).abs().clip(.00001)\n",
    "            \n",
    "        score = f.loc[not_selected] / correlation.loc[not_selected, selected].mean(axis = 1).fillna(.00001)\n",
    "        best = score.index[score.argmax()]\n",
    "        selected.append(best)\n",
    "        not_selected.remove(best)\n",
    "        \n",
    "    feat_list.append(selected)\n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "31\n",
      "46\n",
      "76\n",
      "114\n",
      "136\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "mrmr_feat_list_10_percentile = []\n",
    "mrmr_feat_list_20_percentile = []\n",
    "mrmr_feat_list_30_percentile = []\n",
    "mrmr_feat_list_50_percentile = []\n",
    "mrmr_feat_list_75_percentile = []\n",
    "mrmr_feat_list_90_percentile = []\n",
    "\n",
    "for samples in subsamples:\n",
    "    x_sample = samples[0]\n",
    "    y_sample = samples[1]\n",
    "    \n",
    "    F = pd.Series(f_regression(x_sample, y_sample)[0], index = x_sample.columns)\n",
    "    corr = pd.DataFrame(.001, index = x_sample.columns, columns = x_sample.columns)\n",
    "\n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_10_percentile[0]))\n",
    "    mrmr_feat_list_10_percentile += x\n",
    "    \n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_20_percentile[0]))\n",
    "    mrmr_feat_list_20_percentile += x\n",
    "    \n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_30_percentile[0]))\n",
    "    mrmr_feat_list_30_percentile += x\n",
    "    \n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_50_percentile[0]))\n",
    "    mrmr_feat_list_50_percentile += x\n",
    "    \n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_75_percentile[0]))\n",
    "    mrmr_feat_list_75_percentile += x\n",
    "\n",
    "    x = mrmr_feat_selected(x_sample, y_sample, F, corr, len(mi_feat_list_90_percentile[0]))\n",
    "    mrmr_feat_list_90_percentile += x\n",
    "        \n",
    "print(len(mrmr_feat_list_10_percentile[0]))\n",
    "print(len(mrmr_feat_list_20_percentile[0]))\n",
    "print(len(mrmr_feat_list_30_percentile[0]))\n",
    "print(len(mrmr_feat_list_50_percentile[0]))\n",
    "print(len(mrmr_feat_list_75_percentile[0]))\n",
    "print(len(mrmr_feat_list_90_percentile[0]))\n",
    "\n",
    "print(len(mrmr_feat_list_10_percentile))\n",
    "print(len(mrmr_feat_list_20_percentile))\n",
    "print(len(mrmr_feat_list_30_percentile))\n",
    "print(len(mrmr_feat_list_50_percentile))\n",
    "print(len(mrmr_feat_list_75_percentile))\n",
    "print(len(mrmr_feat_list_90_percentile))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI & mRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "25\n",
      "34\n",
      "60\n",
      "88\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "mi_mrmr_feat_list_10_percentile = []\n",
    "for feat in zip(mi_feat_list_10_percentile, mrmr_feat_list_10_percentile):\n",
    "    mi_mrmr_feat_list_10_percentile.append(list(set(feat[0]).intersection(feat[1])))\n",
    "\n",
    "mi_mrmr_feat_list_20_percentile = []\n",
    "for feat in zip(mi_feat_list_20_percentile, mrmr_feat_list_20_percentile):\n",
    "    mi_mrmr_feat_list_20_percentile.append(list(set(feat[0]).intersection(feat[1])))\n",
    "    \n",
    "mi_mrmr_feat_list_30_percentile = []\n",
    "for feat in zip(mi_feat_list_30_percentile, mrmr_feat_list_30_percentile):\n",
    "    mi_mrmr_feat_list_30_percentile.append(list(set(feat[0]).intersection(feat[1])))\n",
    "    \n",
    "mi_mrmr_feat_list_50_percentile = []\n",
    "for feat in zip(mi_feat_list_50_percentile, mrmr_feat_list_50_percentile):\n",
    "    mi_mrmr_feat_list_50_percentile.append(list(set(feat[0]).intersection(feat[1]))) \n",
    "    \n",
    "mi_mrmr_feat_list_75_percentile = []\n",
    "for feat in zip(mi_feat_list_75_percentile, mrmr_feat_list_75_percentile):\n",
    "    mi_mrmr_feat_list_75_percentile.append(list(set(feat[0]).intersection(feat[1]))) \n",
    "\n",
    "mi_mrmr_feat_list_90_percentile = []\n",
    "for feat in zip(mi_feat_list_90_percentile, mrmr_feat_list_90_percentile):\n",
    "    mi_mrmr_feat_list_90_percentile.append(list(set(feat[0]).intersection(feat[1]))) \n",
    "    \n",
    "print(len(mi_mrmr_feat_list_10_percentile[0]))\n",
    "print(len(mi_mrmr_feat_list_20_percentile[0]))\n",
    "print(len(mi_mrmr_feat_list_30_percentile[0]))\n",
    "print(len(mi_mrmr_feat_list_50_percentile[0]))\n",
    "print(len(mi_mrmr_feat_list_75_percentile[0]))\n",
    "print(len(mi_mrmr_feat_list_90_percentile[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving in CSV and Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for samples in subsamples:\n",
    "    sample_df = pd.concat([samples[0], samples[1]], axis = 1)\n",
    "    sample_df.to_csv('/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/subsamples_v2/{}/subsample_{}_v2.csv'.format(length_index, i))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_of_lists_to_file(list_of_lists, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sublist in list_of_lists:\n",
    "            line = ' '.join(str(element) for element in sublist) + '\\n'\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_of_lists_to_file(mi_feat_list_10_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_10_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_feat_list_20_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_20_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_feat_list_30_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_30_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_feat_list_50_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_50_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_feat_list_75_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_75_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_feat_list_90_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_feat_list_90_percentile_v2.txt'.format(length_index))\n",
    "\n",
    "write_list_of_lists_to_file(mrmr_feat_list_10_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_10_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mrmr_feat_list_20_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_20_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mrmr_feat_list_30_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_30_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mrmr_feat_list_50_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_50_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mrmr_feat_list_75_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_75_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mrmr_feat_list_90_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mrmr_feat_list_90_percentile_v2.txt'.format(length_index))\n",
    "\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_10_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_10_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_20_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_20_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_30_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_30_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_50_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_50_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_75_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_75_percentile_v2.txt'.format(length_index))\n",
    "write_list_of_lists_to_file(mi_mrmr_feat_list_90_percentile, '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/short_trajectory/features_v2/{}/mi_mrmr_feat_list_90_percentile_v2.txt'.format(length_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shopper_Intent_Prediction-jr_Cp9Sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
