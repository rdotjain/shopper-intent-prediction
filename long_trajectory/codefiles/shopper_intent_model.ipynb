{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_dataframes = '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/long_trajectory/subsamples/'\n",
    "directory_features = '/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/long_trajectory/features/'\n",
    "\n",
    "def get_sample_df(directory=directory_dataframes):\n",
    "    list_dataframes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f):\n",
    "            list_dataframes.append(pd.read_csv(f))\n",
    "            \n",
    "    return list_dataframes\n",
    "\n",
    "def get_features(regex_str, directory=directory_features):\n",
    "    regex = re.compile('/Users/nitanshjain/Documents/Projects/Shopper_Intent_Prediction/shopper-intent-prediction/long_trajectory/features/{}'.format(regex_str))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if regex.match(f):\n",
    "            file1 = open(f,\"r+\")\n",
    "            feat_list = file1.read().splitlines()\n",
    "            \n",
    "            #txt file converts everything to string, so we need to convert it back to list\n",
    "            for val in feat_list:\n",
    "                #adding ; to be used a separator for list\n",
    "                new_val = val.replace('y','y;').replace(') ','); ').replace('4 ', '4; ').replace('5 ', '5; ')\n",
    "                feat_list[feat_list.index(val)] = new_val\n",
    "                \n",
    "    for val in feat_list:\n",
    "        #separating the string into a list of features\n",
    "        new_val = val.split('; ')\n",
    "        feat_list[feat_list.index(val)] = new_val\n",
    "        \n",
    "    return feat_list\n",
    "\n",
    "list_sample_dataframes = get_sample_df(directory_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_predict(model, regex_str, dataframes=list_sample_dataframes):\n",
    "    \n",
    "    feat_list = get_features(regex_str)\n",
    "    \n",
    "    accuracy_list = []\n",
    "    f1_score_list = []\n",
    "    auc_list = []\n",
    "    \n",
    "    for sample, feat in zip(dataframes, feat_list):\n",
    "        x = sample[feat]\n",
    "        y = sample['conversion_class']\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        # print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "        f1_score_list.append(f1_score(y_test, y_pred))\n",
    "        auc_list.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    print('Average Accuracy', np.mean(accuracy_list))\n",
    "    print('Average F1 Score', np.mean(f1_score_list))\n",
    "    print('Average AUC', np.mean(auc_list)) \n",
    "    \n",
    "    print('Max Accuracy', max(accuracy_list))\n",
    "    print('Max F1 Score', max(f1_score_list))\n",
    "    print('Max AUC', max(auc_list))  \n",
    "    \n",
    "    best_accuracy_index = accuracy_list.index(max(accuracy_list))\n",
    "    best_f1_score_index = f1_score_list.index(max(f1_score_list))\n",
    "    best_auc_index = auc_list.index(max(auc_list))\n",
    "    \n",
    "    print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n",
    "    print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n",
    "    print('Best Sample Index based on Max AUC', best_auc_index)\n",
    "    \n",
    "    print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n",
    "    print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n",
    "    print('Best Features based on Max AUC', feat_list[best_auc_index]) \n",
    "    \n",
    "     \n",
    "    \n",
    "    return accuracy_list, f1_score_list, auc_list  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to be used for baseline\n",
    "<li>Logistic Regression</li>\n",
    "<li>Random Forest</li>\n",
    "<li>Support Vector Machine</li>\n",
    "<li>XGB</li>\n",
    "<li>Neural Network</li>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8086538461538462\n",
      "Average F1 Score 0.8034728121749242\n",
      "Average AUC 0.8086538461538462\n",
      "Max Accuracy 0.8269230769230769\n",
      "Max F1 Score 0.8181818181818181\n",
      "Max AUC 0.8269230769230769\n",
      "Best Sample Index based on Max Accuracy 2\n",
      "Best Sample Index based on Max F1 Score 2\n",
      "Best Sample Index based on Max AUC 2\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(1, 2, 1)', '(2, 1, 2)', '(2, 3, 1)', '(3, 1, 1)']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(1, 2, 1)', '(2, 1, 2)', '(2, 3, 1)', '(3, 1, 1)']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(1, 2, 1)', '(2, 1, 2)', '(2, 3, 1)', '(3, 1, 1)']\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "accuracy_list_lr_10_mi, f1_score_list_lr_10_mi, auc_list_lr_10_mi = model_train_predict(lr, 'mi_feat_list_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8596153846153844\n",
      "Average F1 Score 0.8636717825660881\n",
      "Average AUC 0.8596153846153844\n",
      "Max Accuracy 0.8894230769230769\n",
      "Max F1 Score 0.8888888888888888\n",
      "Max AUC 0.889423076923077\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 3\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "accuracy_list_rfc_10_mi, f1_score_list_rfc_10_mi, auc_list_rfc_10_mi = model_train_predict(rfc, 'mi_feat_list_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8197115384615385\n",
      "Average F1 Score 0.8240373202310808\n",
      "Average AUC 0.8197115384615385\n",
      "Max Accuracy 0.8413461538461539\n",
      "Max F1 Score 0.8411214953271028\n",
      "Max AUC 0.8413461538461539\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 8\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(4, 1, 1)']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "accuracy_list_svm_10_mi, f1_score_list_svm_10_mi, auc_list_svm_10_mi = model_train_predict(svc, 'mi_feat_list_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8528846153846154\n",
      "Average F1 Score 0.8551270069315375\n",
      "Average AUC 0.8528846153846155\n",
      "Max Accuracy 0.8894230769230769\n",
      "Max F1 Score 0.8878048780487806\n",
      "Max AUC 0.889423076923077\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 3\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', '(2,)', '(3,)', '(4,)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 2, 1)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)']\n"
     ]
    }
   ],
   "source": [
    "xgbc = xgb.XGBClassifier()\n",
    "accuracy_list_xgb_10_mi, f1_score_list_xgb_10_mi, auc_list_xgb_10_mi = model_train_predict(xgbc, 'mi_feat_list_10')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8192307692307692\n",
      "Average F1 Score 0.815936348894392\n",
      "Average AUC 0.8192307692307693\n",
      "Max Accuracy 0.8365384615384616\n",
      "Max F1 Score 0.8316831683168318\n",
      "Max AUC 0.8365384615384616\n",
      "Best Sample Index based on Max Accuracy 5\n",
      "Best Sample Index based on Max F1 Score 5\n",
      "Best Sample Index based on Max AUC 5\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 3)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', 'E5', 'A5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 3)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', 'E5', 'A5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 3)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', 'E5', 'A5']\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "accuracy_list_lr_20_mi, f1_score_list_lr_20_mi, auc_list_lr_20_mi = model_train_predict(lr, 'mi_feat_list_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8682692307692308\n",
      "Average F1 Score 0.8729923989398609\n",
      "Average AUC 0.8682692307692308\n",
      "Max Accuracy 0.9086538461538461\n",
      "Max F1 Score 0.9107981220657276\n",
      "Max AUC 0.9086538461538461\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 3\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)', '(4, 1, 1)', 'C4', 'E5', 'B5', 'G5', 'A5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)', '(4, 1, 1)', 'C4', 'E5', 'B5', 'G5', 'A5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(1, 4, 1)', '(4, 1, 1)', 'C4', 'E5', 'B5', 'G5', 'A5']\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "accuracy_list_rfc_20_mi, f1_score_list_rfc_20_mi, auc_list_rfc_20_mi = model_train_predict(rfc, 'mi_feat_list_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.81875\n",
      "Average F1 Score 0.8221220116526453\n",
      "Average AUC 0.81875\n",
      "Max Accuracy 0.8317307692307693\n",
      "Max F1 Score 0.835680751173709\n",
      "Max AUC 0.8317307692307693\n",
      "Best Sample Index based on Max Accuracy 5\n",
      "Best Sample Index based on Max F1 Score 6\n",
      "Best Sample Index based on Max AUC 6\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(1, 3)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 4)', '(3, 3, 3)', '(4, 1, 1)', '(2, 3, 2)', '(3, 1, 4)', '(4, 4, 2)', 'A4', 'D4', 'C4', 'E5', 'B5', 'N5', 'A5', 'L5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(2, 2, 1)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 2)', '(2, 1, 4)', '(4, 1, 4)', '(1, 3, 3)', '(4, 1, 1)', '(3, 1, 4)', 'A4', 'E5', 'B5', 'Q5', 'V5', 'A5', 'L5', 'S5', 'M5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(2, 2, 1)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 2)', '(2, 1, 4)', '(4, 1, 4)', '(1, 3, 3)', '(4, 1, 1)', '(3, 1, 4)', 'A4', 'E5', 'B5', 'Q5', 'V5', 'A5', 'L5', 'S5', 'M5']\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "accuracy_list_svm_30_mi, f1_score_list_svm_30_mi, auc_list_svm_30_mi = model_train_predict(svc, 'mi_feat_list_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8615384615384617\n",
      "Average F1 Score 0.862320791887624\n",
      "Average AUC 0.8615384615384617\n",
      "Max Accuracy 0.9134615384615384\n",
      "Max F1 Score 0.9126213592233009\n",
      "Max AUC 0.9134615384615385\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 3\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(6,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 2)', '(2, 3)', '(3, 1)', '(6, 3)', '(3, 2)', '(1, 4)', '(4, 1)', '(1, 3)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 2)', '(2, 2, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 1, 4)', '(1, 4, 1)', '(2, 1, 4)', '(1, 3, 3)', '(3, 3, 3)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 2, 3)', '(1, 6, 2)', '(3, 1, 4)', '(2, 3, 3)', '(2, 1, 6)', '(1, 4, 4)', '(6, 2, 3)', '(2, 3, 4)', 'A4', 'D4', 'E4', 'C4', 'F4', 'E5', 'T5', 'O5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'U5', 'A5', 'L5', 'P5', 'H5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(6,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 2)', '(2, 3)', '(3, 1)', '(6, 3)', '(3, 2)', '(1, 4)', '(4, 1)', '(1, 3)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 2)', '(2, 2, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 1, 4)', '(1, 4, 1)', '(2, 1, 4)', '(1, 3, 3)', '(3, 3, 3)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 2, 3)', '(1, 6, 2)', '(3, 1, 4)', '(2, 3, 3)', '(2, 1, 6)', '(1, 4, 4)', '(6, 2, 3)', '(2, 3, 4)', 'A4', 'D4', 'E4', 'C4', 'F4', 'E5', 'T5', 'O5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'U5', 'A5', 'L5', 'P5', 'H5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(6,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 2)', '(2, 3)', '(3, 1)', '(6, 3)', '(3, 2)', '(1, 4)', '(4, 1)', '(1, 3)', '(3, 3)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 2)', '(2, 2, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 1, 4)', '(1, 4, 1)', '(2, 1, 4)', '(1, 3, 3)', '(3, 3, 3)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 2, 3)', '(1, 6, 2)', '(3, 1, 4)', '(2, 3, 3)', '(2, 1, 6)', '(1, 4, 4)', '(6, 2, 3)', '(2, 3, 4)', 'A4', 'D4', 'E4', 'C4', 'F4', 'E5', 'T5', 'O5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'U5', 'A5', 'L5', 'P5', 'H5']\n"
     ]
    }
   ],
   "source": [
    "xgbc = xgb.XGBClassifier()\n",
    "accuracy_list_xgb_50_mi, f1_score_list_xgb_50_mi, auc_list_xgb_50_mi = model_train_predict(xgbc, 'mi_feat_list_50')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8134615384615385\n",
      "Average F1 Score 0.8105844002059858\n",
      "Average AUC 0.8134615384615385\n",
      "Max Accuracy 0.8317307692307693\n",
      "Max F1 Score 0.8258706467661692\n",
      "Max AUC 0.8317307692307693\n",
      "Best Sample Index based on Max Accuracy 7\n",
      "Best Sample Index based on Max F1 Score 7\n",
      "Best Sample Index based on Max AUC 7\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(6, 3)', '(1, 4)', '(4, 1)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 4, 1)', '(4, 1, 2)', '(2, 1, 4)', '(4, 1, 4)', '(1, 3, 3)', '(3, 2, 1)', '(4, 1, 1)', '(3, 1, 4)', 'A4', 'E4', 'C4', 'F4', 'E5', 'B5', 'A5', 'L5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(6, 3)', '(1, 4)', '(4, 1)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 4, 1)', '(4, 1, 2)', '(2, 1, 4)', '(4, 1, 4)', '(1, 3, 3)', '(3, 2, 1)', '(4, 1, 1)', '(3, 1, 4)', 'A4', 'E4', 'C4', 'F4', 'E5', 'B5', 'A5', 'L5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(6, 3)', '(1, 4)', '(4, 1)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 1)', '(2, 6, 3)', '(6, 3, 1)', '(1, 4, 1)', '(4, 1, 2)', '(2, 1, 4)', '(4, 1, 4)', '(1, 3, 3)', '(3, 2, 1)', '(4, 1, 1)', '(3, 1, 4)', 'A4', 'E4', 'C4', 'F4', 'E5', 'B5', 'A5', 'L5']\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "accuracy_list_lr_30_mi, f1_score_list_lr_30_mi, auc_list_lr_30_mi = model_train_predict(lr, 'mi_feat_list_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy 0.8663461538461539\n",
      "Average F1 Score 0.8702828746201291\n",
      "Average AUC 0.8663461538461539\n",
      "Max Accuracy 0.9038461538461539\n",
      "Max F1 Score 0.9056603773584906\n",
      "Max AUC 0.9038461538461539\n",
      "Best Sample Index based on Max Accuracy 3\n",
      "Best Sample Index based on Max F1 Score 3\n",
      "Best Sample Index based on Max AUC 3\n",
      "Best Features based on Max Accuracy ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(3, 3)', '(1, 6)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 1, 4)', 'C4', 'F4', 'E5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'A5', 'P5']\n",
      "Best Features based on Max F1 Score ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(3, 3)', '(1, 6)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 1, 4)', 'C4', 'F4', 'E5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'A5', 'P5']\n",
      "Best Features based on Max AUC ['unigram_entropy', 'bigram_entropy', 'trigram_entropy', 'pattern_hvg_4_nodes_entropy', 'pattern_hvg_5_node_entropy', '(2,)', '(3,)', '(4,)', '(1, 1)', '(1, 2)', '(2, 1)', '(2, 3)', '(3, 1)', '(1, 4)', '(4, 1)', '(3, 3)', '(1, 6)', '(1, 1, 1)', '(1, 1, 2)', '(1, 2, 1)', '(2, 1, 2)', '(1, 2, 3)', '(2, 3, 1)', '(3, 1, 2)', '(2, 2, 3)', '(3, 1, 1)', '(1, 1, 4)', '(1, 4, 1)', '(4, 1, 1)', '(3, 3, 1)', '(2, 3, 2)', '(3, 1, 4)', 'C4', 'F4', 'E5', 'B5', 'N5', 'Q5', 'V5', 'G5', 'A5', 'P5']\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "accuracy_list_rfc_30_mi, f1_score_list_rfc_30_mi, auc_list_rfc_30_mi = model_train_predict(rfc, 'mi_feat_list_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "accuracy_list_svm_30_mi, f1_score_list_svm_30_mi, auc_list_svm_30_mi = model_train_predict(svc, 'mi_feat_list_30')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shopper_Intent_Prediction-jr_Cp9Sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
