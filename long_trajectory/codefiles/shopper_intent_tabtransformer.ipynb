{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.decomposition import *\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_dataframes = '/content/drive/MyDrive/subsamples/'\n",
    "directory_features = '/content/drive/MyDrive/features/'\n",
    "\n",
    "def get_sample_df(directory=directory_dataframes):\n",
    "    list_dataframes = []\n",
    "    filename_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        print(filename)\n",
    "        filename_list.append(filename)\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f):\n",
    "            list_dataframes.append(pd.read_csv(f))\n",
    "            \n",
    "    return list_dataframes, filename_list\n",
    "\n",
    "def get_features(regex_str, directory=directory_features):\n",
    "    regex = re.compile('/content/drive/MyDrive/features/{}'.format(regex_str))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if regex.match(f):\n",
    "            file1 = open(f,\"r+\")\n",
    "            feat_list = file1.read().splitlines()\n",
    "            \n",
    "            #txt file converts everything to string, so we need to convert it back to list\n",
    "            for i in range(len(feat_list)):\n",
    "                #adding ; to be used a separator for list\n",
    "                if i<len(feat_list):\n",
    "                    new_val = feat_list[i].replace('y','y;').replace(') ','); ').replace('4 ', '4; ').replace('5 ', '5; ')\n",
    "                    feat_list[i] = new_val\n",
    "                \n",
    "    for val in feat_list:\n",
    "        #separating the string into a list of features\n",
    "        new_val = val.split('; ')\n",
    "        feat_list[feat_list.index(val)] = new_val\n",
    "        \n",
    "    return feat_list\n",
    "\n",
    "list_sample_dataframes, filename_sample_list = get_sample_df(directory_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_predict_ft(regex_str, dataframes=list_sample_dataframes):\n",
    "    \n",
    "    feat_list = get_features(regex_str)\n",
    "    accuracy_list = []\n",
    "    f1_score_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    for sample, feat in zip(dataframes, feat_list):\n",
    "        feat[len(feat)-1] = feat[len(feat)-1].replace('y;', 'y')\n",
    "        \n",
    "        CATEGORICAL_FEATURES = [] \n",
    "        NUMERIC_FEATURES = feat\n",
    "        FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "        LABEL = 'conversion_class'\n",
    "        # print(FEATURES)\n",
    "        \n",
    "        train_data, test_data = train_test_split(sample, test_size=0.2, random_state=42, stratify=sample['conversion_class'])\n",
    "        \n",
    "        train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n",
    "        test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "        train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n",
    "        test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)\n",
    "        \n",
    "        # # Transform to TF dataset\n",
    "        train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL, batch_size=32)\n",
    "        test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], LABEL, shuffle=False, batch_size=32)\n",
    "        \n",
    "        # print(train_dataset)\n",
    "\n",
    "        ft_linear_encoder = FTTransformerEncoder(\n",
    "            numerical_features = NUMERIC_FEATURES,  # list of numeric features\n",
    "            categorical_features = CATEGORICAL_FEATURES,  # list of numeric features\n",
    "            numerical_data = train_data[NUMERIC_FEATURES],  # pandas dataframe of numeric features\n",
    "            categorical_data = train_data[CATEGORICAL_FEATURES],  # pandas dataframe of categorical features\n",
    "            # categorical_lookup=category_prep_layers,  # dictionary of categorical lookup layers\n",
    "            # numerical_embeddings=None,  # None for linear embeddings\n",
    "            numerical_embedding_type='linear',  # Numerical embedding type\n",
    "            embedding_dim=16,  # Embedding dimension (for categorical, numerical, and contextual)\n",
    "            depth=3,  # Number of Transformer Blocks (layers)\n",
    "            heads=6,  # Number of attention heads in a Transofrmer Block\n",
    "            attn_dropout=0.2,  # Dropout for attention layers\n",
    "            ff_dropout=0.2,  # Dropout in Dense layers\n",
    "            # use_column_embedding=True,  # Fixed column embeddings\n",
    "            explainable=True  # Whether we want to output attention importances or not\n",
    "        )\n",
    "\n",
    "        # Pass the encoder to the model\n",
    "        ft_linear_transformer = FTTransformer(\n",
    "            encoder=ft_linear_encoder,  # Encoder from above\n",
    "            out_dim=1,  # Number of outputs in final layer\n",
    "            out_activation='sigmoid',  # Activation function for final layer\n",
    "            # final_layer_size=32,  # Pre-final layer, takes CLS contextual embeddings as input \n",
    "        )\n",
    "        \n",
    "        LEARNING_RATE = 0.0001\n",
    "        WEIGHT_DECAY = 0.0001\n",
    "        NUM_EPOCHS = 10\n",
    "\n",
    "        # Define optimised\n",
    "        optimizer = tfa.optimizers.AdamW(\n",
    "                learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "            )\n",
    "\n",
    "        # Two outputs, second output is importances, so we don't calculate loss for it\n",
    "        ft_linear_transformer.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss = {\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n",
    "            metrics= {\"output\": [tf.keras.metrics.AUC(name=\"PR AUC\", curve='PR')], \"importances\": None},\n",
    "        )\n",
    "\n",
    "        # Training\n",
    "        ft_linear_history = ft_linear_transformer.fit(\n",
    "            train_dataset, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            verbose=0          \n",
    "        )\n",
    "        \n",
    "        \n",
    "        linear_test_preds = ft_linear_transformer.predict(test_dataset)\n",
    "        auc_list.append(np.round(roc_auc_score(test_data[LABEL], linear_test_preds['output'].ravel()), 4))\n",
    "        f1_score_list.append(np.round(f1_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n",
    "        accuracy_list.append(np.round(accuracy_score(test_data[LABEL], linear_test_preds['output'].ravel()>0.5), 4))\n",
    "\n",
    "        # print('Average Accuracy', np.mean(accuracy_list))\n",
    "        # print('Average F1 Score', np.mean(f1_score_list))\n",
    "        # print('Average AUC', np.mean(auc_list)) \n",
    "        \n",
    "        # print('Max Accuracy', max(accuracy_list))\n",
    "        # print('Max F1 Score', max(f1_score_list))\n",
    "        # print('Max AUC', max(auc_list))  \n",
    "        \n",
    "        # best_accuracy_index = accuracy_list.index(max(accuracy_list))\n",
    "        # best_f1_score_index = f1_score_list.index(max(f1_score_list))\n",
    "        # best_auc_index = auc_list.index(max(auc_list))\n",
    "        \n",
    "        # print('Best Sample Index based on Max Accuracy', best_accuracy_index)\n",
    "        # print('Best Sample Index based on Max F1 Score', best_f1_score_index)\n",
    "        # print('Best Sample Index based on Max AUC', best_auc_index)\n",
    "        \n",
    "        # print('Best Features based on Max Accuracy', feat_list[best_accuracy_index])\n",
    "        # print('Best Features based on Max F1 Score', feat_list[best_f1_score_index])\n",
    "        # print('Best Features based on Max AUC', feat_list[best_auc_index]) \n",
    "\n",
    "    return accuracy_list, f1_score_list, auc_list,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_10_mi, f1_score_list_10_mi, auc_list_10_mi = model_train_predict_ft('mi_feat_list_10*',)\n",
    "accuracy_list_20_mi, f1_score_list_20_mi, auc_list_20_mi = model_train_predict_ft('mi_feat_list_20*',)\n",
    "accuracy_list_30_mi, f1_score_list_30_mi, auc_list_30_mi = model_train_predict_ft('mi_feat_list_30*',)\n",
    "accuracy_list_50_mi, f1_score_list_50_mi, auc_list_50_mi = model_train_predict_ft('mi_feat_list_50*',)\n",
    "accuracy_list_75_mi, f1_score_list_75_mi, auc_list_75_mi = model_train_predict_ft('mi_feat_list_75*',)\n",
    "accuracy_list_90_mi, f1_score_list_90_mi, auc_list_90_mi = model_train_predict_ft('mi_feat_list_90*',)\n",
    "\n",
    "\n",
    "accuracy_list_10_mrmr, f1_score_list_10_mrmr, auc_list_10_mrmr = model_train_predict_ft('mrmr_feat_list_10*',)\n",
    "accuracy_list_20_mrmr, f1_score_list_20_mrmr, auc_list_20_mrmr = model_train_predict_ft('mrmr_feat_list_20*',)\n",
    "accuracy_list_30_mrmr, f1_score_list_30_mrmr, auc_list_30_mrmr = model_train_predict_ft('mrmr_feat_list_30*',)\n",
    "accuracy_list_50_mrmr, f1_score_list_50_mrmr, auc_list_50_mrmr = model_train_predict_ft('mrmr_feat_list_50*',)\n",
    "accuracy_list_75_mrmr, f1_score_list_75_mrmr, auc_list_75_mrmr = model_train_predict_ft('mrmr_feat_list_75*',)\n",
    "accuracy_list_90_mrmr, f1_score_list_90_mrmr, auc_list_90_mrmr = model_train_predict_ft('mrmr_feat_list_90*',)\n",
    "\n",
    "\n",
    "accuracy_list_10_mi_mrmr, f1_score_list_10_mi_mrmr, auc_list_10_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_10*',)\n",
    "accuracy_list_20_mi_mrmr, f1_score_list_20_mi_mrmr, auc_list_20_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_20*',)\n",
    "accuracy_list_30_mi_mrmr, f1_score_list_30_mi_mrmr, auc_list_30_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_30*',)\n",
    "accuracy_list_50_mi_mrmr, f1_score_list_50_mi_mrmr, auc_list_50_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_50*',)\n",
    "accuracy_list_75_mi_mrmr, f1_score_list_75_mi_mrmr, auc_list_75_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_75*',)\n",
    "accuracy_list_90_mi_mrmr, f1_score_list_90_mi_mrmr, auc_list_90_mi_mrmr = model_train_predict_ft('mi_mrmr_feat_list_90*',)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopper-intent-prediction-L5e-nQUd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
